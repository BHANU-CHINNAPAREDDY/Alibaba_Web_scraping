# -*- coding: utf-8 -*-
"""alibaba_web_scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14rqdUYdMF4JILgOqWgSsqe7JbZfcTf9-
"""

# !pip install selenium

# !apt-get install -y chromium-browser

# !apt install chromium-chromedriver

# !wget -q -O chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
# !dpkg -i chrome.deb || apt --fix-broken install -y

# !apt-get install -y unzip xvfb libxi6 libgconf-2-4

# !wget -q https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip
# !unzip chromedriver_linux64.zip
# !mv chromedriver /usr/bin/chromedriver
# !chmod +x /usr/bin/chromedriver

# !pip install webdriver-manager selenium

!ls /usr/lib/chromium-browser/chromedriver

path = "/usr/lib/chromium-browser/chromedriver"

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from concurrent.futures import ProcessPoolExecutor, as_completed
import time, html, json
import pandas as pd
from math import ceil

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

def get_total_pages():
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # service = Service(executable_path="/usr/bin/chromedriver")
    # Auto-manage the correct chromedriver version for the current Chrome
    service = Service(ChromeDriverManager().install())
    browser = webdriver.Chrome(service=service, options=options)
    browser.get("https://sourcing.alibaba.com/rfq/rfq_search_list.htm?country=AE&recently=Y")

    try:
        pagination_div = browser.find_element(By.ID, 'pagination-example')
        links = pagination_div.find_elements(By.TAG_NAME, 'a')
        pages = [int(link.text) for link in links if link.text.isdigit()]
        return max(pages) if pages else 1
    except Exception as e:
        print("Error while fetching page count:", e)
        return 1
    finally:
        browser.quit()

# print(get_total_pages())

def divide_page_ranges(total_pages, num_chunks=5):
    num_chunks = min(total_pages, num_chunks)  # Avoid more chunks than pages
    step = total_pages // num_chunks
    remainder = total_pages % num_chunks

    ranges = []
    start = 1

    for i in range(num_chunks):
        end = start + step - 1
        if remainder > 0:
            end += 1
            remainder -= 1
        ranges.append((start, end))
        start = end + 1

    return ranges

def scrape_range(start_page, end_page):
    import time
    import json, html
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager

    # Setup Chrome options
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # Setup WebDriver
    # path = "C:\\chromedriver.exe"
    service = Service(ChromeDriverManager().install())
    # service = Service(executable_path=path)
    browser = webdriver.Chrome(service=service, options=options)

    extracted_data = []

    for page in range(start_page, end_page + 1):
        try:
            url = f"https://sourcing.alibaba.com/rfq/rfq_search_list.htm?country=AE&recently=Y&page={page}"
            browser.get(url)
            # time.sleep(2)

            rfq_cards_list = browser.find_elements(By.CSS_SELECTOR, ".brh-rfq-item.brh-rfq-item--r1c3.alife-bc-brh-rfq-list__item")

            print(f"Scraping Page {page} â€” Found {len(rfq_cards_list)} RFQs")

            for rfq_card in rfq_cards_list:
                original_window = browser.current_window_handle

                # Extract tags
                try:
                    tags = rfq_card.find_element(By.CLASS_NAME, "next-tag-body").text.lower()
                    email_confirmed = "Yes" if "email" in tags else "No"
                    experienced_buyer = "Yes" if "experienced" in tags else "No"
                    complete_order_via_rfq = "Yes" if "complete" in tags else "No"
                    typical_replies = "Yes" if "typical" in tags else "No"
                    interactive_user = "Yes" if "interactive" in tags else "No"
                except:
                    email_confirmed = experienced_buyer = complete_order_via_rfq = typical_replies = interactive_user = ""

                try:
                    inquiry_time = rfq_card.find_element(By.CLASS_NAME, 'brh-rfq-item__publishtime').text
                except:
                    inquiry_time = ""

                try:
                    inquiry_url = rfq_card.find_element(By.CLASS_NAME, "brh-rfq-item__subject-link").get_attribute("href")
                except:
                    inquiry_url = ""
                # Opening the details tab
                try:
                    title_element = rfq_card.find_element(By.CLASS_NAME, "brh-rfq-item__subject-link")
                    title = title_element.text
                    before_click = browser.window_handles
                    title_element.click()
                    # time.sleep(2)
                    after_click = browser.window_handles
                    new_tab = [h for h in after_click if h not in before_click][0]
                    browser.switch_to.window(new_tab)
                except:
                    title = ""
                    continue

                try:
                    title = browser.find_element(By.CLASS_NAME, 'topic-title').text
                except:
                    title = ""

                try:
                    buyer_name = browser.find_element(By.CLASS_NAME, 'buyer-name').text
                except:
                    buyer_name = ""

                try:
                    buyer_img_div = browser.find_element(By.CLASS_NAME, "buyer-img")
                    buyer_image_tag = buyer_img_div.find_element(By.TAG_NAME, 'img')
                    buyer_image = buyer_image_tag.get_attribute("src")
                except:
                    buyer_image = ""

                try:
                    quotes_left = browser.find_element(By.CLASS_NAME, "left").text
                except:
                    quotes_left = ""

                try:
                    country_name = browser.find_element(By.CLASS_NAME, "country-name").text
                except:
                    country_name = ""

                try:
                    quantity_required = browser.find_element(By.CLASS_NAME, "tp-count").text
                except:
                    quantity_required = ""

                try:
                    inquiry_date = browser.find_element(By.CLASS_NAME, "datetime").text
                except:
                    inquiry_date = ""

                scraping_date = time.strftime("%Y-%m-%d", time.localtime())

                try:
                    rfqid_span_element = browser.find_element(By.ID, "report")
                    raw_data = rfqid_span_element.get_attribute('data-emogine-record')
                    parsed_data = json.loads(html.unescape(raw_data))
                    rfqid = parsed_data.get("rfqId", "")
                except:
                    rfqid = ""

                data = {
                    "RFQ ID": rfqid,
                    "Title": title,
                    "Buyer Name": buyer_name,
                    "Buyer Image": buyer_image,
                    "Inquiry Time": inquiry_time,
                    "Quotes Left": quotes_left,
                    "Country": country_name,
                    "Quantity Required": quantity_required,
                    "Email Confirmed": email_confirmed,
                    "Experienced Buyer": experienced_buyer,
                    "Complete Order via RFQ": complete_order_via_rfq,
                    "Typical Replies": typical_replies,
                    "Interactive User": interactive_user,
                    "Inquiry URL": inquiry_url,
                    "Inquiry Date": inquiry_date,
                    "Scraping Date": scraping_date
                }

                extracted_data.append(data)

                # Close the new tab and return to original
                try:
                    if len(browser.window_handles) > 1:
                        browser.close()
                        browser.switch_to.window(original_window)
                except:
                    pass

        except Exception as e:
            print(f"[Page {page}] Error: {e}")
            continue

    browser.quit()
    return extracted_data

# result = scrape_range(0,0)

# df = pd.DataFrame(result)
# df

from concurrent.futures import ThreadPoolExecutor, as_completed

total_pages = get_total_pages()
print(total_pages)
page_ranges = divide_page_ranges(total_pages, num_chunks=10)
print(page_ranges)
all_results = []

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(scrape_range, start, end) for start, end in page_ranges]

    for future in as_completed(futures):
        try:
            result = future.result()
            all_results.extend(result)
        except Exception as e:
            print("Error during scraping task:", e)

df = pd.DataFrame(all_results)

# To store the extracted info without doing any filters
dff = pd.DataFrame(all_results)
dff.to_csv('alibaba_rfq_unfiltered.csv', index=False)

df['Inquiry Time'] = df['Inquiry Time'].str.split('\n').str[-1].str.strip()
df['Quotes Left'] = df['Quotes Left'].str.split(" ").str[-1].str.strip()
df['Quantity Required'] = df['Quantity Required'].str.split("\n").str[-1].str.strip()
df['Email Confirmed'] = df['Email Confirmed'].apply(lambda x: 'No' if str(x).strip() == '' else x)
df['Experienced Buyer'] =  df['Experienced Buyer'].apply(lambda x: 'No' if str(x).strip() == '' else x)
df['Complete Order via RFQ'] = df['Complete Order via RFQ'].apply(lambda x: 'No' if str(x).strip() == '' else x)
df['Typical Replies'] = df['Typical Replies'].apply(lambda x: 'No' if str(x).strip() == '' else x)
df['Interactive User'] = df['Interactive User'].apply(lambda x: 'No' if str(x).strip() == '' else x)
df['Inquiry Date'] = df['Inquiry Date'].str.split().str[0]

# df.iloc[0,8]
df

# Sorting the dataframe so that the recent rfqs at top.

df_1 = df.copy()
import numpy as np
# Sample data
# df['Inquiry Time'] might contain: "Just now", "9 minutes before", "1 hours before", "2 days before"

def convert_inquiry_time_to_minutes(text):
    text = text.lower().strip()

    if "just now" in text:
        return 0
    elif "minutes" in text:
        return int(text.split()[0])
    elif "minute" in text:
        return int(text.split()[0])
    elif "hours" in text:
        return int(text.split()[0]) * 60
    elif "hour" in text:
        return int(text.split()[0]) * 60
    elif "days" in text:
        return int(text.split()[0]) * 1440
    elif "day" in text:
        return int(text.split()[0]) * 1440
    else:
        return np.nan  # unknown or empty values

# Create a sortable numeric column
df_1['InquiryTimeInMinutes'] = df_1['Inquiry Time'].apply(convert_inquiry_time_to_minutes)

# Sort DataFrame by the calculated minutes column (ascending = past to recent, so reverse that)
df_1 = df_1.sort_values('InquiryTimeInMinutes')

# Drop helper column if not needed
df_1 = df_1.drop(columns='InquiryTimeInMinutes').reset_index(drop=True)

df_1



df_1.to_csv('alibaba_rfq_final.csv', index=False)

